{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c0e6dc3-b22a-464c-b5cf-1f6a30bdd223",
   "metadata": {},
   "source": [
    "# First look at Spark/PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb763af",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b575d75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "648e6c81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "876c352a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/spark/python/pyspark/__init__.py'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark.__file__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed2b97b",
   "metadata": {},
   "source": [
    "## Instantiate a Spark session\n",
    "\n",
    "We need to instantiate a **SparkSession**, an object that we use to interact with Spark.\n",
    "\n",
    "```\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"test\") \\\n",
    "    .getOrCreate()\n",
    "```\n",
    "* **`SparkSession`** class is the entry point into all functionality in Spark. To create a SparkSession, use the **`builder`** attribute.\n",
    "* **`master()`** sets the Spark master URL to connect to. The `local` string means that Spark will run on a local cluster. `[*]` means that Spark will run with as many CPU cores as possible.\n",
    "* **`appName()`** sets a name for the application/session, which will be shown in the Spark web UI.\n",
    "* **`getOrCreate()`** gets an existing SparkSession or, if there is no existing one, creates a new one based on the options set in this builder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "234707a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/22 08:52:13 WARN Utils: Your hostname, GRAD0365UBUNTU resolves to a loopback address: 127.0.1.1; using 10.5.4.63 instead (on interface wlp0s20f3)\n",
      "24/02/22 08:52:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/22 08:52:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/02/22 08:52:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"test\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c4bb41-35f8-4d98-b9ab-32970017c34b",
   "metadata": {},
   "source": [
    "Once we've instantiated a session, we can access the Spark UI by browsing to **`localhost:4040`**. The UI will display all current jobs. Since we've just created the instance, there should be no jobs currently running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddb3f54",
   "metadata": {},
   "source": [
    "## Read a CSV file\n",
    "\n",
    "For this example we will use the [High Volume For-Hire Vehicle Trip Records for January 2021](https://github.com/DataTalksClub/nyc-tlc-data/releases/tag/fhvhv) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b7edee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-02-22 08:52:15--  https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/fhvhv_tripdata_2021-01.csv.gz\n",
      "Resolviendo github.com (github.com)... 140.82.121.3\n",
      "Conectando con github.com (github.com)[140.82.121.3]:443... conectado.\n",
      "Petición HTTP enviada, esperando respuesta... 302 Found\n",
      "Ubicación: https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/035746e8-4e24-47e8-a3ce-edcf6d1b11c7?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240222%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240222T075215Z&X-Amz-Expires=300&X-Amz-Signature=5969715ea17004a87d69d86fda7f0eb3eb0327c525509e1c77c700bee87aaed1&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dfhvhv_tripdata_2021-01.csv.gz&response-content-type=application%2Foctet-stream [siguiente]\n",
      "--2024-02-22 08:52:15--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/035746e8-4e24-47e8-a3ce-edcf6d1b11c7?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240222%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240222T075215Z&X-Amz-Expires=300&X-Amz-Signature=5969715ea17004a87d69d86fda7f0eb3eb0327c525509e1c77c700bee87aaed1&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dfhvhv_tripdata_2021-01.csv.gz&response-content-type=application%2Foctet-stream\n",
      "Resolviendo objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Conectando con objects.githubusercontent.com (objects.githubusercontent.com)[185.199.111.133]:443... conectado.\n",
      "Petición HTTP enviada, esperando respuesta... 200 OK\n",
      "Longitud: 129967421 (124M) [application/octet-stream]\n",
      "Guardando como: “../data/fhvhv_tripdata_2021-01.csv.gz”\n",
      "\n",
      "../data/fhvhv_tripd 100%[===================>] 123,95M  19,3MB/s    en 6,5s    \n",
      "\n",
      "2024-02-22 08:52:22 (19,0 MB/s) - “../data/fhvhv_tripdata_2021-01.csv.gz” guardado [129967421/129967421]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/fhvhv_tripdata_2021-01.csv.gz -O ../data/fhvhv_tripdata_2021-01.csv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19efeae7-b292-4c27-a29f-839bc6e282c3",
   "metadata": {},
   "source": [
    "Similarly to Pandas, Spark can read CSV files into **dataframes**, a tabular data structure. Unlike Pandas, Spark can handle much bigger datasets but it's unable to infer the datatypes of each column.\n",
    "\n",
    "Let's read the file and create a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43713ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"../data/fhvhv_tripdata_2021-01.csv.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b587720-9121-461d-8791-206b89c338aa",
   "metadata": {},
   "source": [
    "* **`read`** reads the file.\n",
    "* **`option()`** contains options for the `read` method. In this case, we're specifying that the first line of the CSV file contains the column names.\n",
    "* **`csv()`** is for reading CSV files.\n",
    "\n",
    "We can see the contents of the dataframe with **`df.show()`** (only a few rows will be shown) or **`df.head()`**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2a6ca74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|           HV0003|              B02682|2021-01-01 00:33:44|2021-01-01 00:49:07|         230|         166|   null|\n",
      "|           HV0003|              B02682|2021-01-01 00:55:19|2021-01-01 01:18:21|         152|         167|   null|\n",
      "|           HV0003|              B02764|2021-01-01 00:23:56|2021-01-01 00:38:05|         233|         142|   null|\n",
      "|           HV0003|              B02764|2021-01-01 00:42:51|2021-01-01 00:45:50|         142|         143|   null|\n",
      "|           HV0003|              B02764|2021-01-01 00:48:14|2021-01-01 01:08:42|         143|          78|   null|\n",
      "|           HV0005|              B02510|2021-01-01 00:06:59|2021-01-01 00:43:01|          88|          42|   null|\n",
      "|           HV0005|              B02510|2021-01-01 00:50:00|2021-01-01 01:04:57|          42|         151|   null|\n",
      "|           HV0003|              B02764|2021-01-01 00:14:30|2021-01-01 00:50:27|          71|         226|   null|\n",
      "|           HV0003|              B02875|2021-01-01 00:22:54|2021-01-01 00:30:20|         112|         255|   null|\n",
      "|           HV0003|              B02875|2021-01-01 00:40:12|2021-01-01 00:53:31|         255|         232|   null|\n",
      "|           HV0003|              B02875|2021-01-01 00:56:45|2021-01-01 01:17:42|         232|         198|   null|\n",
      "|           HV0003|              B02835|2021-01-01 00:29:04|2021-01-01 00:36:27|         113|          48|   null|\n",
      "|           HV0003|              B02835|2021-01-01 00:48:56|2021-01-01 00:59:12|         239|          75|   null|\n",
      "|           HV0004|              B02800|2021-01-01 00:15:24|2021-01-01 00:38:31|         181|         237|   null|\n",
      "|           HV0004|              B02800|2021-01-01 00:45:00|2021-01-01 01:06:45|         236|          68|   null|\n",
      "|           HV0003|              B02682|2021-01-01 00:11:53|2021-01-01 00:18:06|         256|         148|   null|\n",
      "|           HV0003|              B02682|2021-01-01 00:28:31|2021-01-01 00:41:40|          79|          80|   null|\n",
      "|           HV0003|              B02682|2021-01-01 00:50:49|2021-01-01 00:55:59|          17|         217|   null|\n",
      "|           HV0005|              B02510|2021-01-01 00:08:40|2021-01-01 00:39:39|          62|          29|   null|\n",
      "|           HV0003|              B02836|2021-01-01 00:53:48|2021-01-01 01:11:40|          22|          22|   null|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86006d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(hvfhs_license_num='HV0003', dispatching_base_num='B02682', pickup_datetime='2021-01-01 00:33:44', dropoff_datetime='2021-01-01 00:49:07', PULocationID='230', DOLocationID='166', SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02682', pickup_datetime='2021-01-01 00:55:19', dropoff_datetime='2021-01-01 01:18:21', PULocationID='152', DOLocationID='167', SR_Flag=None)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d43f1e-b67b-42c5-a73a-d0a96a9dd0b3",
   "metadata": {},
   "source": [
    "We can also check the current schema with **`df.printSchema()`**; you will notice that all values are strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cfaf1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- dropoff_datetime: string (nullable = true)\n",
      " |-- PULocationID: string (nullable = true)\n",
      " |-- DOLocationID: string (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51a6fc2",
   "metadata": {},
   "source": [
    "### Infer datatypes\n",
    "\n",
    "We can use a trick with Pandas to infer the datatypes:\n",
    "1. Create a smaller CSV file with the first 100 records.\n",
    "2. Import Pandas and create a Pandas dataframe. This dataframe will have inferred datatypes.\n",
    "3. Create a Spark dataframe from the Pandas dataframe and check its schema.\n",
    "```py\n",
    "spark.createDataFrame(df_pandas).schema\n",
    "```\n",
    "4. Based on the output of the previous method, import the datatypes needed from **`pyspark.sql.types`**, and create a **`StructType`** object containing a list of the datatypes. **`types`** contains all of the available data types for Spark dataframes.\n",
    "```py\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType\n",
    "schema = StructType([StructField(...), StructField(...), ...])\n",
    "```\n",
    "5. Create a new Spark dataframe and include the schema as an option.\n",
    "```py\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv(\"../data/fhvhv_tripdata_2021-01.csv.gz\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb69c0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gzip: ../data/fhvhv_tripdata_2021-01.csv already exists; do you wish to overwrite (y or n)? ^C\n"
     ]
    }
   ],
   "source": [
    "!gzip -dk \"../data/fhvhv_tripdata_2021-01.csv.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b75eb311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hvfhs_license_num,dispatching_base_num,pickup_datetime,dropoff_datetime,PULocationID,DOLocationID,SR_Flag\n",
      "HV0003,B02682,2021-01-01 00:33:44,2021-01-01 00:49:07,230,166,\n",
      "HV0003,B02682,2021-01-01 00:55:19,2021-01-01 01:18:21,152,167,\n",
      "HV0003,B02764,2021-01-01 00:23:56,2021-01-01 00:38:05,233,142,\n",
      "HV0003,B02764,2021-01-01 00:42:51,2021-01-01 00:45:50,142,143,\n"
     ]
    }
   ],
   "source": [
    "!head -n 101 \"../data/fhvhv_tripdata_2021-01.csv\" > \"../data/head.csv\"\n",
    "!head -n 5 \"../data/head.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "921fb729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 ../data/head.csv\n"
     ]
    }
   ],
   "source": [
    "!wc -l \"../data/head.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1792b13d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hvfhs_license_num        object\n",
       "dispatching_base_num     object\n",
       "pickup_datetime          object\n",
       "dropoff_datetime         object\n",
       "PULocationID              int64\n",
       "DOLocationID              int64\n",
       "SR_Flag                 float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_pandas = pd.read_csv(\"../data/head.csv\")\n",
    "df_pandas.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc20f8fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('hvfhs_license_num', StringType(), True), StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', StringType(), True), StructField('dropoff_datetime', StringType(), True), StructField('PULocationID', LongType(), True), StructField('DOLocationID', LongType(), True), StructField('SR_Flag', DoubleType(), True)])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# workaround to avoid AttributeError: 'DataFrame' object has no attribute 'iteritems'\n",
    "pd.DataFrame.iteritems = pd.DataFrame.items\n",
    "\n",
    "spark.createDataFrame(df_pandas).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90398a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify the previous output to have the data types as we want\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType \n",
    "\n",
    "schema = StructType([\n",
    "        StructField('hvfhs_license_num', StringType(), True),\n",
    "        StructField('dispatching_base_num', StringType(), True), \n",
    "        StructField('pickup_datetime', TimestampType(), True), \n",
    "        StructField('dropoff_datetime', TimestampType(), True), \n",
    "        StructField('PULocationID', IntegerType(), True), \n",
    "        StructField('DOLocationID', IntegerType(), True), \n",
    "        StructField('SR_Flag', StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d67090e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv(\"../data/fhvhv_tripdata_2021-01.csv.gz\")\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f43c8b",
   "metadata": {},
   "source": [
    "### Infer datatypes: a quicker option\n",
    "\n",
    "Another option, which saves us all the previous steps, is to set the **`inferSchema`** option to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0af713e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .csv(\"../data/fhvhv_tripdata_2021-01.csv.gz\")\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94caf141",
   "metadata": {},
   "source": [
    "## Partitions / saving to parquet files\n",
    "\n",
    "A **Spark cluster** is composed of multiple **executors**. Each executor can process data independently in order to parallelize and speed up work.\n",
    "\n",
    "In the previous example we read a single large CSV file. This file can only be read by a single executor, which means that the rest of the executors will be idle and we will not take advantage of parallelization. \n",
    "\n",
    "To avoid this we can split a file into multiple parts so that each executor can take care of a part and have all executors working simultaneously. These splits are called **partitions**.\n",
    "\n",
    "In our example, we partition the dataframe that we obtained when reading the CSV file, and save the result in parquet format. This will create multiple parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "568a612b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/22 08:54:14 WARN MemoryManager: Total allocation exceeds 95,00% (960.285.889 bytes) of heap memory\n",
      "Scaling row group sizes to 89,43% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:===================>                                      (8 + 8) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/22 08:54:16 WARN MemoryManager: Total allocation exceeds 95,00% (960.285.889 bytes) of heap memory\n",
      "Scaling row group sizes to 89,43% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=================================>                       (14 + 8) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/22 08:54:17 WARN MemoryManager: Total allocation exceeds 95,00% (960.285.889 bytes) of heap memory\n",
      "Scaling row group sizes to 89,43% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# create 24 partitions of the dataframe\n",
    "df = df.repartition(24)\n",
    "\n",
    "# save the partitions in parquet format\n",
    "df.write.parquet(\"../data/fhvhv/2021/01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2debc37-6d18-409b-abcc-d5acb4b09c88",
   "metadata": {},
   "source": [
    "We can check in the Spark UI the progress of our jobs, which are divided into stages that contain tasks. The tasks in a stage will not start until all tasks in the previous stage are finished.\n",
    "\n",
    "When creating a dataframe, Spark produces as many partitions as CPU cores available by default, and each partition creates a task. \n",
    "\n",
    "Besides the 24 parquet files, you should also see a `_SUCCESS` file which should be empty. This file is created to indicate that the job have finished successfully.\n",
    "\n",
    "Trying to write the files again will output an error because Spark will not write to a non-empty folder. You can force an overwrite with the `mode` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef5be638-48fa-462d-b0d7-9303aaf6a8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/22 08:54:46 WARN MemoryManager: Total allocation exceeds 95,00% (960.285.889 bytes) of heap memory\n",
      "Scaling row group sizes to 89,43% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:================>                                        (7 + 8) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/22 08:54:47 WARN MemoryManager: Total allocation exceeds 95,00% (960.285.889 bytes) of heap memory\n",
      "Scaling row group sizes to 89,43% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:==============================>                         (13 + 8) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/22 08:54:49 WARN MemoryManager: Total allocation exceeds 95,00% (960.285.889 bytes) of heap memory\n",
      "Scaling row group sizes to 89,43% for 8 writers\n",
      "24/02/22 08:54:49 WARN MemoryManager: Total allocation exceeds 95,00% (960.285.889 bytes) of heap memory\n",
      "Scaling row group sizes to 89,43% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.parquet(\"../data/fhvhv/2021/01/\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afab34c-737c-4503-8c86-85099836fa0b",
   "metadata": {},
   "source": [
    "The opposite of partitioning (joining multiple partitions into a single partition) is called **coalescing**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa127727",
   "metadata": {},
   "source": [
    "## Read parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23e38ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"../data/fhvhv/2021/01\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e24e83-c4ca-4e86-8f01-de6496037eb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
