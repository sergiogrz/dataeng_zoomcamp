{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2018c65f-247a-4200-8895-b8a0b0b4fe39",
   "metadata": {},
   "source": [
    "# Module 5 Homework Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0ad831-e5c2-4176-91a7-43970112acfe",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "754001bd-1680-44e5-8033-ebfd1f168427",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e0b894-a271-409e-9685-5ce0f2494f94",
   "metadata": {},
   "source": [
    "## Create a local Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7c7ede5-970f-4d9b-a123-85adde311dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/25 18:12:43 WARN Utils: Your hostname, GRAD0365UBUNTU resolves to a loopback address: 127.0.1.1; using 192.168.68.103 instead (on interface wlp0s20f3)\n",
      "24/02/25 18:12:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/25 18:12:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"spark homework\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355ecee9-3910-45fc-b247-f9d5af28dff1",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "**Execute `spark.version`. What's the output?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "188abbc7-ff34-4e07-b07b-eb554510e0bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6039b98e-9e2b-4aec-bcba-20f75e6acd33",
   "metadata": {},
   "source": [
    "## Read the data and save it to Parquet\n",
    "\n",
    "For this homework we will be using the FHV 2019-10 data found here: [FHV Data](https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-10.csv.gz).\n",
    "\n",
    "Read the data into a Spark DataFrame with a schema as we did in the lessons. Repartition the DataFrame to 6 partitions and save it to Parquet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f97d72-8ab2-40df-bcc4-5e07635d51e4",
   "metadata": {},
   "source": [
    "### Download the CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a23c70b5-a9fe-41cc-ae2d-a8816db6345a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: no se puede crear el directorio «../data/fhv»: El archivo ya existe\n",
      "--2024-02-25 18:12:45--  https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-10.csv.gz\n",
      "Resolviendo github.com (github.com)... 140.82.121.3\n",
      "Conectando con github.com (github.com)[140.82.121.3]:443... conectado.\n",
      "Petición HTTP enviada, esperando respuesta... 302 Found\n",
      "Ubicación: https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/efdfcf82-6d5c-44d1-a138-4e8ea3c3a3b6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240225%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240225T171245Z&X-Amz-Expires=300&X-Amz-Signature=644da50ef163d179731b5fa95c155ba77ac3ef2578e0a45f0072ee2b0e2e4418&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dfhv_tripdata_2019-10.csv.gz&response-content-type=application%2Foctet-stream [siguiente]\n",
      "--2024-02-25 18:12:46--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/efdfcf82-6d5c-44d1-a138-4e8ea3c3a3b6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240225%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240225T171245Z&X-Amz-Expires=300&X-Amz-Signature=644da50ef163d179731b5fa95c155ba77ac3ef2578e0a45f0072ee2b0e2e4418&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dfhv_tripdata_2019-10.csv.gz&response-content-type=application%2Foctet-stream\n",
      "Resolviendo objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Conectando con objects.githubusercontent.com (objects.githubusercontent.com)[185.199.108.133]:443... conectado.\n",
      "Petición HTTP enviada, esperando respuesta... 200 OK\n",
      "Longitud: 19375751 (18M) [application/octet-stream]\n",
      "Guardando como: “../data/fhv/fhv_tripdata_2019-10.csv.gz”\n",
      "\n",
      "../data/fhv/fhv_tri 100%[===================>]  18,48M  1,89MB/s    en 10s     \n",
      "\n",
      "2024-02-25 18:12:57 (1,80 MB/s) - “../data/fhv/fhv_tripdata_2019-10.csv.gz” guardado [19375751/19375751]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir ../data/fhv\n",
    "!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-10.csv.gz \\\n",
    "    -O ../data/fhv/fhv_tripdata_2019-10.csv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8c7aa2-32fb-4da3-94c9-8d319ede9a27",
   "metadata": {},
   "source": [
    "### Infer datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "932cfbec-1d77-4348-b5f5-f145f6ef5307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dispatching_base_num       object\n",
       "pickup_datetime            object\n",
       "dropOff_datetime           object\n",
       "PUlocationID              float64\n",
       "DOlocationID              float64\n",
       "SR_Flag                   float64\n",
       "Affiliated_base_number     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas = pd.read_csv(\"../data/fhv/fhv_tripdata_2019-10.csv.gz\", nrows=1000)\n",
    "df_pandas.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad7f8b49-1274-4f3c-bb36-07df5bb0f365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dispatching_base_num</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropOff_datetime</th>\n",
       "      <th>PUlocationID</th>\n",
       "      <th>DOlocationID</th>\n",
       "      <th>SR_Flag</th>\n",
       "      <th>Affiliated_base_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B00009</td>\n",
       "      <td>2019-10-01 00:23:00</td>\n",
       "      <td>2019-10-01 00:35:00</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00013</td>\n",
       "      <td>2019-10-01 00:11:29</td>\n",
       "      <td>2019-10-01 00:13:22</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00014</td>\n",
       "      <td>2019-10-01 00:11:43</td>\n",
       "      <td>2019-10-01 00:37:20</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B00014</td>\n",
       "      <td>2019-10-01 00:56:29</td>\n",
       "      <td>2019-10-01 00:57:47</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00014</td>\n",
       "      <td>2019-10-01 00:23:09</td>\n",
       "      <td>2019-10-01 00:28:27</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dispatching_base_num      pickup_datetime     dropOff_datetime  \\\n",
       "0               B00009  2019-10-01 00:23:00  2019-10-01 00:35:00   \n",
       "1               B00013  2019-10-01 00:11:29  2019-10-01 00:13:22   \n",
       "2               B00014  2019-10-01 00:11:43  2019-10-01 00:37:20   \n",
       "3               B00014  2019-10-01 00:56:29  2019-10-01 00:57:47   \n",
       "4               B00014  2019-10-01 00:23:09  2019-10-01 00:28:27   \n",
       "\n",
       "   PUlocationID  DOlocationID  SR_Flag Affiliated_base_number  \n",
       "0         264.0         264.0      NaN                 B00009  \n",
       "1         264.0         264.0      NaN                 B00013  \n",
       "2         264.0         264.0      NaN                 B00014  \n",
       "3         264.0         264.0      NaN                 B00014  \n",
       "4         264.0         264.0      NaN                 B00014  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e563b75-02b7-4285-b12f-8fc4f10bc6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 7 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   dispatching_base_num    1000 non-null   object \n",
      " 1   pickup_datetime         1000 non-null   object \n",
      " 2   dropOff_datetime        1000 non-null   object \n",
      " 3   PUlocationID            999 non-null    float64\n",
      " 4   DOlocationID            999 non-null    float64\n",
      " 5   SR_Flag                 0 non-null      float64\n",
      " 6   Affiliated_base_number  997 non-null    object \n",
      "dtypes: float64(3), object(4)\n",
      "memory usage: 54.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df_pandas.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1b5120e-afe5-4118-84f2-9ce28d46ecd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a schema for the dataframe\n",
    "schema = StructType([\n",
    "        StructField(\"dispatching_base_num\", StringType(), True),\n",
    "        StructField(\"pickup_datetime\", TimestampType(), True), \n",
    "        StructField(\"dropoff_datetime\", TimestampType(), True), \n",
    "        StructField(\"PULocationID\", IntegerType(), True), \n",
    "        StructField(\"DOLocationID\", IntegerType(), True), \n",
    "        StructField(\"SR_Flag\", StringType(), True),\n",
    "        StructField(\"Affiliated_base_number\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfd033b1-41b0-4231-8cb8-92ca5d7458d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the data as a Spark DataFrame using the schema we want\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv(\"../data/fhv/fhv_tripdata_2019-10.csv.gz\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e5eb265-9997-48ea-a9f7-b28a9abd6545",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# repartition the DataFrame and save it to Parquet\n",
    "df.repartition(6).write.parquet(\"../data/fhv/2019/10\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbcd514-d013-484e-b731-3a2c371820ae",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "**What is the average size of the Parquet (ending with .parquet extension) Files that were created (in MB)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c74354ed-5366-4515-b06e-e39b3001314e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 sgrodriguez usuarios del dominio 6,4M feb 25 18:13 ../data/fhv/2019/10/part-00000-556011c9-d8d5-4c60-9bec-3138a466bbe7-c000.snappy.parquet\n",
      "-rw-r--r-- 1 sgrodriguez usuarios del dominio 6,4M feb 25 18:13 ../data/fhv/2019/10/part-00001-556011c9-d8d5-4c60-9bec-3138a466bbe7-c000.snappy.parquet\n",
      "-rw-r--r-- 1 sgrodriguez usuarios del dominio 6,4M feb 25 18:13 ../data/fhv/2019/10/part-00002-556011c9-d8d5-4c60-9bec-3138a466bbe7-c000.snappy.parquet\n",
      "-rw-r--r-- 1 sgrodriguez usuarios del dominio 6,4M feb 25 18:13 ../data/fhv/2019/10/part-00003-556011c9-d8d5-4c60-9bec-3138a466bbe7-c000.snappy.parquet\n",
      "-rw-r--r-- 1 sgrodriguez usuarios del dominio 6,4M feb 25 18:13 ../data/fhv/2019/10/part-00004-556011c9-d8d5-4c60-9bec-3138a466bbe7-c000.snappy.parquet\n",
      "-rw-r--r-- 1 sgrodriguez usuarios del dominio 6,4M feb 25 18:13 ../data/fhv/2019/10/part-00005-556011c9-d8d5-4c60-9bec-3138a466bbe7-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ../data/fhv/2019/10/*.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c052df54-b0da-45d2-9386-bcf17b11a1a1",
   "metadata": {},
   "source": [
    "**Answer:** 6,4MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7ae29f-cd75-4ad8-a288-a523736f8816",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "**How many taxi trips were there on the 15th of October? Consider only trips that started on the 15th of October.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a6272b6-ea23-4738-9c35-b63171063326",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "62610"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# via Spark DataFrame\n",
    "df \\\n",
    "    .withColumn(\"pickup_date\", F.to_date(df.pickup_datetime)) \\\n",
    "    .filter(F.col(\"pickup_date\") == \"2019-10-15\") \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1dfb6f5-32fd-400d-bf56-36e3712eb504",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|num_trips|\n",
      "+---------+\n",
      "|    62610|\n",
      "+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# via SparkSQL\n",
    "df.createOrReplaceTempView(\"fhv_table\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(1) AS num_trips\n",
    "FROM fhv_table\n",
    "WHERE TO_DATE(pickup_datetime) = '2019-10-15';\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5727c8b-1a9f-4176-9c59-6e8afb091526",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "**What is the length of the longest trip in the dataset in hours?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0984a61-c439-4f09-952a-de1bc92a87e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------------+\n",
      "|pickup_date|longest_trip_duration_h|\n",
      "+-----------+-----------------------+\n",
      "| 2019-10-28|               631152.5|\n",
      "| 2019-10-11|               631152.5|\n",
      "| 2019-10-31|      87672.44083333333|\n",
      "| 2019-10-01|      70128.02805555555|\n",
      "| 2019-10-17|                 8794.0|\n",
      "+-----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# via Spark DataFrame\n",
    "df \\\n",
    "    .withColumn(\"trip_duration_h\", (F.unix_timestamp(df.dropoff_datetime) - F.unix_timestamp(df.pickup_datetime)) / 3600) \\\n",
    "    .withColumn(\"pickup_date\", F.to_date(df.pickup_datetime)) \\\n",
    "    .select(\"pickup_date\", \"trip_duration_h\") \\\n",
    "    .groupBy(\"pickup_date\") \\\n",
    "    .agg(F.max(\"trip_duration_h\").alias(\"longest_trip_duration_h\")) \\\n",
    "    .orderBy(\"longest_trip_duration_h\", ascending=False) \\\n",
    "    .limit(5) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f0a3907-21c7-4d8d-b5be-59bb5bed38b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------------+\n",
      "|pickup_date|longest_trip_duration_h|\n",
      "+-----------+-----------------------+\n",
      "| 2019-10-28|               631152.5|\n",
      "| 2019-10-11|               631152.5|\n",
      "| 2019-10-31|      87672.44083333333|\n",
      "| 2019-10-01|      70128.02805555555|\n",
      "| 2019-10-17|                 8794.0|\n",
      "+-----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# via SparkSQL\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    TO_DATE(pickup_datetime) AS pickup_date,\n",
    "    MAX((UNIX_TIMESTAMP(dropoff_datetime) - UNIX_TIMESTAMP(pickup_datetime)) / 3600) AS longest_trip_duration_h\n",
    "FROM fhv_table\n",
    "GROUP BY pickup_date\n",
    "ORDER BY longest_trip_duration_h DESC\n",
    "LIMIT 5;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12711bfb-9355-409c-a0c8-384e345660f2",
   "metadata": {},
   "source": [
    "**Answer:** 631,152.5 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484cc341-78b0-4296-939a-5ed71a94e03e",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "**Spark’s User Interface which shows the application's dashboard runs on which local port?**\n",
    "\n",
    "**Answer:** 4040 (`localhost:4040`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c68bc76-608a-4e61-ab03-e3e8b661e2ce",
   "metadata": {},
   "source": [
    "## Join two tables\n",
    "\n",
    "Load the zone lookup data into a temp view in Spark: [Zone Data](https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c34631b-a110-4316-a391-b5857794ff99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download zone lookup data\n",
    "# !wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv -O ../data/taxi_zone_lookup.csv\n",
    "\n",
    "# read data into a dataframe\n",
    "df_zones = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .csv(\"../data/taxi_zone_lookup.csv\")\n",
    "df_zones.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4f1ddc",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "**Using the zone lookup data and the FHV October 2019 data, what is the name of the LEAST frequent pickup location Zone?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53cf6a96-be93-4e96-959d-4d8bb9d8183a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                Zone|count|\n",
      "+--------------------+-----+\n",
      "|         Jamaica Bay|    1|\n",
      "|Governor's Island...|    2|\n",
      "| Green-Wood Cemetery|    5|\n",
      "|       Broad Channel|    8|\n",
      "|     Highbridge Park|   14|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# join both tables\n",
    "df.join(df_zones, on=df[\"PUlocationID\"] == df_zones[\"LocationID\"], how=\"left\") \\\n",
    "    .groupBy(\"Zone\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"count\", ascending=True) \\\n",
    "    .limit(5) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5dd1f19d-807d-40b5-b55d-6c22d50fcdb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|                zone|count_trips|\n",
      "+--------------------+-----------+\n",
      "|         Jamaica Bay|          1|\n",
      "|Governor's Island...|          2|\n",
      "| Green-Wood Cemetery|          5|\n",
      "|       Broad Channel|          8|\n",
      "|     Highbridge Park|         14|\n",
      "+--------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# via Spark SQL\n",
    "df_zones.createOrReplaceTempView(\"zones\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    zones.Zone AS zone,\n",
    "    COUNT(1) as count_trips\n",
    "FROM fhv_table\n",
    "LEFT JOIN zones\n",
    "ON fhv_table.PUlocationID = zones.LocationID\n",
    "GROUP BY zone\n",
    "ORDER BY count_trips ASC\n",
    "LIMIT 5;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce0676c-db64-4022-9345-4cfd4583d02c",
   "metadata": {},
   "source": [
    "**Answer:** Jamaica Bay (1 trip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bca6fa-8a29-4e02-a2f2-258db75929f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
