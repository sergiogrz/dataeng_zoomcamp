{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c07e481a-0414-41b4-bc7c-f58c898c8465",
   "metadata": {},
   "source": [
    "# GroupBy and joins in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5b698c-4d62-4aea-8a24-d6861f5757e8",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfa01f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9789d702-1ba6-4619-90c1-c1879ac9c621",
   "metadata": {},
   "source": [
    "## Create a Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab95d319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/23 13:09:48 WARN Utils: Your hostname, GRAD0365UBUNTU resolves to a loopback address: 127.0.1.1; using 192.168.68.103 instead (on interface wlp0s20f3)\n",
      "24/02/23 13:09:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/23 13:09:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"groupBy and joins\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbc371a",
   "metadata": {},
   "source": [
    "## GroupBy\n",
    "### Green taxi dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c2347fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- lpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- lpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- ehail_fee: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- trip_type: integer (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_green = spark.read.parquet(\"../data/pq/green/*/*\")\n",
    "df_green.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ede83711",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green.createOrReplaceTempView(\"green_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65669ea5-9661-41a1-b19a-5e2c96b21a32",
   "metadata": {},
   "source": [
    "Let's imagine we have the following query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41085ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green_revenue = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    date_trunc('hour', lpep_pickup_datetime) AS hour,\n",
    "    PULocationID AS revenue_zone,\n",
    "    SUM(total_amount) AS amount,\n",
    "    COUNT(1) AS number_records\n",
    "FROM\n",
    "    green_data\n",
    "WHERE lpep_pickup_datetime >= '2020-01-01 00:00:00'\n",
    "GROUP BY\n",
    "    1, 2\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "024d047a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+-----------------+--------------+\n",
      "|               hour|revenue_zone|           amount|number_records|\n",
      "+-------------------+------------+-----------------+--------------+\n",
      "|2020-01-24 09:00:00|          81|            59.49|             2|\n",
      "|2020-01-04 21:00:00|          25|           513.83|            32|\n",
      "|2020-01-10 19:00:00|          66|545.6800000000001|            27|\n",
      "|2020-01-30 07:00:00|          75|556.6600000000001|            40|\n",
      "|2020-01-18 01:00:00|         260|           144.56|            12|\n",
      "+-------------------+------------+-----------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_green_revenue.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91b0e645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/23 13:16:47 WARN MemoryManager: Total allocation exceeds 95,00% (906.992.014 bytes) of heap memory\n",
      "Scaling row group sizes to 96,54% for 7 writers\n",
      "24/02/23 13:16:47 WARN MemoryManager: Total allocation exceeds 95,00% (906.992.014 bytes) of heap memory\n",
      "Scaling row group sizes to 84,47% for 8 writers\n",
      "24/02/23 13:16:47 WARN MemoryManager: Total allocation exceeds 95,00% (906.992.014 bytes) of heap memory\n",
      "Scaling row group sizes to 96,54% for 7 writers\n",
      "24/02/23 13:16:47 WARN MemoryManager: Total allocation exceeds 95,00% (906.992.014 bytes) of heap memory\n",
      "Scaling row group sizes to 96,54% for 7 writers\n",
      "24/02/23 13:16:47 WARN MemoryManager: Total allocation exceeds 95,00% (906.992.014 bytes) of heap memory\n",
      "Scaling row group sizes to 84,47% for 8 writers\n",
      "24/02/23 13:16:47 WARN MemoryManager: Total allocation exceeds 95,00% (906.992.014 bytes) of heap memory\n",
      "Scaling row group sizes to 96,54% for 7 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_green_revenue \\\n",
    "    .repartition(20) \\\n",
    "    .write.parquet(\"../data/report/revenue/green\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0787627-400c-4535-80f8-3e29b8c2a53f",
   "metadata": {},
   "source": [
    "This query outputs the total revenue and amount of trips per hour per zone, so we need to group by hour and zones to get the desired results.\n",
    "\n",
    "On the one hand, we need to group data which is split into different partitions. On the other hand, we have a cluster with some executors, which can only process an individual partition at a time. Spark solves this issue by separating the grouping in 2 stages:\n",
    "\n",
    "1. In the first stage, each executor performs the _GROUP BY_ operation within the partition, and outputs the result to a temporary partition. These temporary partitions are the **intermediate results**.\n",
    "2. In the second stage, we combine the intermediate results by **reshuffling** the data: Spark puts all the records with the same keys (in our example, the _GROUP BY_ keys, which are _hour_ and _revenue\\_zone_) in the same partition. The algorithm that does this is called ***sort merge***. Once the shuffling has finished, Spark applies the _GROUP BY_ again to these new partitions, and combines (**reduces**) the records to the final output.\n",
    "    * NOTE: after reshuffling the data, one partition may contain more than one key, but all the records with the same key must be in the same partition.\n",
    "\n",
    "Shuffling is an expensive operation, so generally we would want to reduce the amount of data to shuffle when querying."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e5fd83",
   "metadata": {},
   "source": [
    "### Yellow taxi dataset\n",
    "\n",
    "Bellow we apply the same operations that we just did, but for Yellow Taxi data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf08e0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_yellow = spark.read.parquet(\"../data/pq/yellow/*/*\")\n",
    "df_yellow.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fa2a04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow.createOrReplaceTempView(\"yellow_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "360afe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow_revenue = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    date_trunc('hour', tpep_pickup_datetime) AS hour,\n",
    "    PULocationID AS revenue_zone,\n",
    "    SUM(total_amount) AS amount,\n",
    "    COUNT(1) AS number_records\n",
    "FROM\n",
    "    yellow_data\n",
    "WHERE tpep_pickup_datetime >= '2020-01-01 00:00:00'\n",
    "GROUP BY\n",
    "    1, 2\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e692f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:===================================================>    (11 + 1) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/23 13:21:56 WARN MemoryManager: Total allocation exceeds 95,00% (906.992.014 bytes) of heap memory\n",
      "Scaling row group sizes to 96,54% for 7 writers\n",
      "24/02/23 13:21:56 WARN MemoryManager: Total allocation exceeds 95,00% (906.992.014 bytes) of heap memory\n",
      "Scaling row group sizes to 84,47% for 8 writers\n",
      "24/02/23 13:21:56 WARN MemoryManager: Total allocation exceeds 95,00% (906.992.014 bytes) of heap memory\n",
      "Scaling row group sizes to 96,54% for 7 writers\n",
      "24/02/23 13:21:56 WARN MemoryManager: Total allocation exceeds 95,00% (906.992.014 bytes) of heap memory\n",
      "Scaling row group sizes to 96,54% for 7 writers\n",
      "24/02/23 13:21:56 WARN MemoryManager: Total allocation exceeds 95,00% (906.992.014 bytes) of heap memory\n",
      "Scaling row group sizes to 84,47% for 8 writers\n",
      "24/02/23 13:21:56 WARN MemoryManager: Total allocation exceeds 95,00% (906.992.014 bytes) of heap memory\n",
      "Scaling row group sizes to 96,54% for 7 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_yellow_revenue \\\n",
    "    .repartition(20) \\\n",
    "    .write.parquet(\"../data/report/revenue/yellow\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4560ea6",
   "metadata": {},
   "source": [
    "## Joins\n",
    "\n",
    "Spark uses similar mechanisms for _GROUP BY_ and for _JOIN_ operations. But we can differenciate two cases: joining two large tables and joining a large table and a small table. \n",
    "\n",
    "### Joining two large tables\n",
    "\n",
    "We now want to join the two tables we've just created by hour and by zone. So, before the join, we create two temporary tables renaming some columns, to be able to differenciate them in the final table. Then, we perform an **outer join** to include records that are only in one of the temporary dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "329867aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green_revenue_tmp = df_green_revenue \\\n",
    "    .withColumnRenamed(\"amount\", \"green_amount\") \\\n",
    "    .withColumnRenamed(\"number_records\", \"green_number_records\")\n",
    "\n",
    "df_yellow_revenue_tmp = df_yellow_revenue \\\n",
    "    .withColumnRenamed(\"amount\", \"yellow_amount\") \\\n",
    "    .withColumnRenamed(\"number_records\", \"yellow_number_records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e1ebce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join = df_green_revenue_tmp.join(df_yellow_revenue_tmp, on=[\"hour\", \"revenue_zone\"], how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "524c8ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[hour: timestamp, revenue_zone: int, green_amount: double, green_number_records: bigint, yellow_amount: double, yellow_number_records: bigint]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb659414-8176-4127-9c68-8ce17714115c",
   "metadata": {},
   "source": [
    "Once we trigger an action (`show()`, `write()`, ...) on `df_join`, Spark creates the two temporary dataframes and the final join dataframe. If we check the DAG in the Spark UI we will see three stages:\n",
    "* Stages 1 and 2 belong to the creation of `df_green_revenue_tmp` and `df_yellow_revenue_tmp`.\n",
    "* For stage 3, let's name `Y1, Y2, ..., Yn` the yellow taxi records, `G1, G2, ..., Gm` the green taxi records, and `K = (hour H, revenue_zone Z)` our composite key. So we can express the records as `(Kn, Yn)` for yellow taxi and `(Km, Gm)` for green taxi. In the beginning of this stage, we have yellow taxi records and green taxi records distributed along their own partitions (that means, in a partition we do not have yet mixed yellow and green taxi records). So Spark **reshuffles** the data, using the **sort merge join algorithm**, to make sure that records with the same key are the same partition, and then it **reduces** the records by joining yellow and green taxi data with the same keys in one final record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "016e31df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:===================================================>    (11 + 1) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+------------------+--------------------+------------------+---------------------+\n",
      "|               hour|revenue_zone|      green_amount|green_number_records|     yellow_amount|yellow_number_records|\n",
      "+-------------------+------------+------------------+--------------------+------------------+---------------------+\n",
      "|2020-01-01 00:00:00|          22|              15.8|                   1|              null|                 null|\n",
      "|2020-01-01 00:00:00|          25|             531.0|                  26|            324.35|                   16|\n",
      "|2020-01-01 00:00:00|          55|129.29000000000002|                   4|              null|                 null|\n",
      "|2020-01-01 00:00:00|          56|             99.69|                   3|              18.1|                    2|\n",
      "|2020-01-01 00:00:00|          60|            160.04|                   6|57.620000000000005|                    2|\n",
      "+-------------------+------------+------------------+--------------------+------------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_join.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0618ebba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:===================================================>    (11 + 1) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/23 17:31:33 WARN MemoryManager: Total allocation exceeds 95,00% (906.992.014 bytes) of heap memory\n",
      "Scaling row group sizes to 96,54% for 7 writers\n",
      "24/02/23 17:31:33 WARN MemoryManager: Total allocation exceeds 95,00% (906.992.014 bytes) of heap memory\n",
      "Scaling row group sizes to 84,47% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:>                                                         (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/23 17:31:34 WARN MemoryManager: Total allocation exceeds 95,00% (906.992.014 bytes) of heap memory\n",
      "Scaling row group sizes to 96,54% for 7 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_join.write.parquet(\"../data/report/revenue/total\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5a84ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+------------------+--------------------+------------------+---------------------+\n",
      "|               hour|revenue_zone|      green_amount|green_number_records|     yellow_amount|yellow_number_records|\n",
      "+-------------------+------------+------------------+--------------------+------------------+---------------------+\n",
      "|2020-01-01 00:00:00|          17|            195.03|                   9|220.20999999999998|                    8|\n",
      "|2020-01-01 00:00:00|          18|               7.8|                   1|               5.8|                    1|\n",
      "|2020-01-01 00:00:00|          24|              87.6|                   3|            754.95|                   45|\n",
      "|2020-01-01 00:00:00|          32| 68.94999999999999|                   2|              18.0|                    1|\n",
      "|2020-01-01 00:00:00|          49|266.76000000000005|                  14|            185.65|                   10|\n",
      "+-------------------+------------+------------------+--------------------+------------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_join = spark.read.parquet(\"../data/report/revenue/total\")\n",
    "df_join.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a54a24",
   "metadata": {},
   "source": [
    "### Joining a large and a small table\n",
    "\n",
    "We are going to use the `zones` lookup table to identify the `revenue_zone` from `df_join` table.\n",
    "\n",
    "In this case, Spark performs **broadcasting**, instead of **sort merge**. It does it that way because the `zones` table is very small. Spark _broadcasts_ or sends a copy of the entire table to each of the executors, and then each executor joins each partition of the big table in memory by performing a lookup on the local broadcasted table. That way there is no need for reshuffling, and this greatly speeds up the join operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb8b298a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-02-23 17:34:24--  https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv\n",
      "Resolviendo github.com (github.com)... 140.82.121.3\n",
      "Conectando con github.com (github.com)[140.82.121.3]:443... conectado.\n",
      "Petición HTTP enviada, esperando respuesta... 302 Found\n",
      "Ubicación: https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/5a2cc2f5-b4cd-4584-9c62-a6ea97ed0e6a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240223%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240223T163424Z&X-Amz-Expires=300&X-Amz-Signature=4e254bc4eac9e528c7d6dc32ffbac549c999c6afbc326015be380e736cc6d34b&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dtaxi_zone_lookup.csv&response-content-type=application%2Foctet-stream [siguiente]\n",
      "--2024-02-23 17:34:25--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/5a2cc2f5-b4cd-4584-9c62-a6ea97ed0e6a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240223%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240223T163424Z&X-Amz-Expires=300&X-Amz-Signature=4e254bc4eac9e528c7d6dc32ffbac549c999c6afbc326015be380e736cc6d34b&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dtaxi_zone_lookup.csv&response-content-type=application%2Foctet-stream\n",
      "Resolviendo objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Conectando con objects.githubusercontent.com (objects.githubusercontent.com)[185.199.109.133]:443... conectado.\n",
      "Petición HTTP enviada, esperando respuesta... 200 OK\n",
      "Longitud: 12322 (12K) [application/octet-stream]\n",
      "Guardando como: “../data/taxi_zone_lookup.csv”\n",
      "\n",
      "../data/taxi_zone_l 100%[===================>]  12,03K  --.-KB/s    en 0,02s   \n",
      "\n",
      "2024-02-23 17:34:25 (597 KB/s) - “../data/taxi_zone_lookup.csv” guardado [12322/12322]\n",
      "\n",
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv -O \"../data/taxi_zone_lookup.csv\"\n",
    "\n",
    "df_zones = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .csv(\"../data/taxi_zone_lookup.csv\")\n",
    "df_zones.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ecf66be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+--------------------+------------------+---------------------+---------+--------------------+------------+\n",
      "|               hour|      green_amount|green_number_records|     yellow_amount|yellow_number_records|  Borough|                Zone|service_zone|\n",
      "+-------------------+------------------+--------------------+------------------+---------------------+---------+--------------------+------------+\n",
      "|2020-01-01 00:00:00|            195.03|                   9|220.20999999999998|                    8| Brooklyn|             Bedford|   Boro Zone|\n",
      "|2020-01-01 00:00:00|               7.8|                   1|               5.8|                    1|    Bronx|        Bedford Park|   Boro Zone|\n",
      "|2020-01-01 00:00:00|              87.6|                   3|            754.95|                   45|Manhattan|        Bloomingdale| Yellow Zone|\n",
      "|2020-01-01 00:00:00| 68.94999999999999|                   2|              18.0|                    1|    Bronx|           Bronxdale|   Boro Zone|\n",
      "|2020-01-01 00:00:00|266.76000000000005|                  14|            185.65|                   10| Brooklyn|        Clinton Hill|   Boro Zone|\n",
      "|2020-01-01 00:00:00|              11.3|                   1|             48.16|                    2| Brooklyn|Flatbush/Ditmas Park|   Boro Zone|\n",
      "|2020-01-01 00:00:00|              null|                null|210.27999999999997|                    3|   Queens|Flushing Meadows-...|   Boro Zone|\n",
      "|2020-01-01 00:00:00|              null|                null|              18.8|                    1| Brooklyn|           Gravesend|   Boro Zone|\n",
      "|2020-01-01 00:00:00| 99.36999999999999|                   6|110.74000000000001|                    7|   Queens|Long Island City/...|   Boro Zone|\n",
      "|2020-01-01 00:00:00| 82.00999999999999|                   5|              20.1|                    2|    Bronx|Mott Haven/Port M...|   Boro Zone|\n",
      "+-------------------+------------------+--------------------+------------------+---------------------+---------+--------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_result = df_join.join(df_zones, on=df_join.revenue_zone == df_zones.LocationID).drop(\"LocationID\", \"revenue_zone\")\n",
    "\n",
    "df_result.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83894d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/23 17:40:37 WARN MemoryManager: Total allocation exceeds 95,00% (906.992.014 bytes) of heap memory\n",
      "Scaling row group sizes to 96,54% for 7 writers\n",
      "24/02/23 17:40:37 WARN MemoryManager: Total allocation exceeds 95,00% (906.992.014 bytes) of heap memory\n",
      "Scaling row group sizes to 84,47% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:==============>                                           (2 + 6) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/23 17:40:37 WARN MemoryManager: Total allocation exceeds 95,00% (906.992.014 bytes) of heap memory\n",
      "Scaling row group sizes to 96,54% for 7 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_result.drop(\"LocationID\", \"revenue_zone\").write.parquet(\"../data/tmp/revenue-zones\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8fb21691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hour: timestamp (nullable = true)\n",
      " |-- green_amount: double (nullable = true)\n",
      " |-- green_number_records: long (nullable = true)\n",
      " |-- yellow_amount: double (nullable = true)\n",
      " |-- yellow_number_records: long (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Zone: string (nullable = true)\n",
      " |-- service_zone: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_result = spark.read.parquet(\"../data/tmp/revenue-zones\")\n",
    "df_result.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f025a720",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
