{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b94960a-1c7b-4f03-ac12-896256c083ef",
   "metadata": {},
   "source": [
    "# Spark DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c596eb65-9ec8-4de3-9a51-90fecaca3a50",
   "metadata": {},
   "source": [
    "We can read as a dataframe the parquet files we have created in the previpus section. Parquet contains the information about schema, so, unlike with CSV, we do not need to specify or infer it when reading the files. This is one of the reasons parquet files are smaller than CSVs: since they \"know\" the schema they use more efficient ways of compressing the data (for example, storing as integers instead of long values)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefcaec0-87a8-4ffc-8e95-7c443df71683",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88614bf0-cb43-4eb3-b8ba-7fecc1049323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f71ba8-2d68-4c9f-91b6-eabd1f8770ab",
   "metadata": {},
   "source": [
    "## Create a Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e940e7e5-51de-4fac-b4b6-a2e1cdee969c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/22 08:39:43 WARN Utils: Your hostname, GRAD0365UBUNTU resolves to a loopback address: 127.0.1.1; using 10.5.4.63 instead (on interface wlp0s20f3)\n",
      "24/02/22 08:39:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/22 08:39:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"test\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8110dd86-8b71-446e-9a51-f750353290f0",
   "metadata": {},
   "source": [
    "## Read (partitioned) parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95c7ab4b-a822-4db8-a239-cc1a42b759c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"../data/fhvhv/2021/01/\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cba4a1-aa6a-40ea-ba5c-ca695ec4f06d",
   "metadata": {},
   "source": [
    "We can apply many Pandas-like operations to Spark dataframes.\n",
    "* If we want to select a few columns, we use **`select()`** method.\n",
    "* To filter by some value, we use **`filter()`**.\n",
    "* We can find some of the many other operations that we can do with Spark in [this quickstart guide from the official Spark documentation](https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "679bd25e-0381-434e-8ecb-942cd4e5b6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+-------------------+------------+\n",
      "|    pickup_datetime|PULocationID|   dropoff_datetime|DOLocationID|\n",
      "+-------------------+------------+-------------------+------------+\n",
      "|2021-01-16 21:58:50|         256|2021-01-16 22:16:27|         177|\n",
      "|2021-01-17 19:36:28|         256|2021-01-17 19:52:28|          49|\n",
      "|2021-01-16 20:19:14|         256|2021-01-16 20:33:52|          36|\n",
      "|2021-01-11 10:42:29|         256|2021-01-11 10:46:44|         217|\n",
      "|2021-01-30 00:23:44|         256|2021-01-30 00:42:09|         168|\n",
      "|2021-01-14 15:55:47|         256|2021-01-14 16:03:43|         255|\n",
      "|2021-01-10 19:11:19|         256|2021-01-10 19:32:45|          36|\n",
      "|2021-01-21 16:18:24|         256|2021-01-21 16:31:47|          34|\n",
      "|2021-01-04 07:03:43|         256|2021-01-04 07:20:17|         162|\n",
      "|2021-01-10 00:55:17|         256|2021-01-10 01:04:55|         144|\n",
      "|2021-01-01 01:14:38|         256|2021-01-01 01:17:29|         255|\n",
      "|2021-01-16 12:11:12|         256|2021-01-16 12:29:12|         211|\n",
      "|2021-01-23 22:24:56|         256|2021-01-23 23:05:06|         265|\n",
      "|2021-01-07 12:07:19|         256|2021-01-07 12:13:19|         255|\n",
      "|2021-01-10 22:28:58|         256|2021-01-10 22:32:56|          17|\n",
      "|2021-01-02 00:54:33|         256|2021-01-02 01:03:45|          97|\n",
      "|2021-01-21 01:39:28|         256|2021-01-21 01:51:21|         198|\n",
      "|2021-01-08 17:08:16|         256|2021-01-08 17:30:14|          79|\n",
      "|2021-01-13 22:57:21|         256|2021-01-13 23:10:16|         181|\n",
      "|2021-01-29 21:59:41|         256|2021-01-29 22:01:46|         255|\n",
      "+-------------------+------------+-------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"pickup_datetime\", \"PULocationID\", \"dropoff_datetime\", \"DOLocationID\") \\\n",
    "    .filter(df.PULocationID == 256) \\\n",
    "    .show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53119f63-ca77-4c6e-9b55-b42dd87479a7",
   "metadata": {},
   "source": [
    "## Actions vs transformations\n",
    "\n",
    "Spark uses a programming concept called **lazy evaluation**. Before Spark does anything with the data in your program, it first builds step-by-step directions of what functions and data it will need. Spark builds the **DAG** from your code, and checks if it can procrastinate, waiting until the last possible moment to get the data. So for example, in the code above, Spark does not really do any job until we call the **`show()`** method.\n",
    "\n",
    "In Spark we differenciate between actions and transformations:\n",
    "* **Transformations:** lazy operations, which are not executed right away. These are operations we use for transforming the data, such as:\n",
    "  * Selecting columns.\n",
    "  * Filtering.\n",
    "  * Joins.\n",
    "  * Group by operations.\n",
    "  * Partitions.\n",
    "  * ...\n",
    "* **Actions:** eager operations, those that are executed right away. Computations only happen when an action is triggered, so then the job will have to perform all of the transformations that lead to that action to produce a value. Examples of actions are:\n",
    "  * Show, take, head.\n",
    "  * Write, read.\n",
    "  * ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "426f4c2d-4f74-49ff-8d8d-47d55d417e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[pickup_datetime: timestamp, PULocationID: int, dropoff_datetime: timestamp, DOLocationID: int]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transformation select()\n",
    "df.select(\"pickup_datetime\", \"PULocationID\", \"dropoff_datetime\", \"DOLocationID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2486296d-3714-4d4b-9286-3659dfb8e08f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[pickup_datetime: timestamp, PULocationID: int, dropoff_datetime: timestamp, DOLocationID: int]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transformations select() + filter()\n",
    "df.select(\"pickup_datetime\", \"PULocationID\", \"dropoff_datetime\", \"DOLocationID\") \\\n",
    "    .filter(df.PULocationID == 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27b20f67-969d-4fcf-beb1-325898de92c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+-------------------+------------+\n",
      "|    pickup_datetime|PULocationID|   dropoff_datetime|DOLocationID|\n",
      "+-------------------+------------+-------------------+------------+\n",
      "|2021-01-16 21:58:50|         256|2021-01-16 22:16:27|         177|\n",
      "|2021-01-17 19:36:28|         256|2021-01-17 19:52:28|          49|\n",
      "|2021-01-16 20:19:14|         256|2021-01-16 20:33:52|          36|\n",
      "|2021-01-11 10:42:29|         256|2021-01-11 10:46:44|         217|\n",
      "|2021-01-30 00:23:44|         256|2021-01-30 00:42:09|         168|\n",
      "|2021-01-14 15:55:47|         256|2021-01-14 16:03:43|         255|\n",
      "|2021-01-10 19:11:19|         256|2021-01-10 19:32:45|          36|\n",
      "|2021-01-21 16:18:24|         256|2021-01-21 16:31:47|          34|\n",
      "|2021-01-04 07:03:43|         256|2021-01-04 07:20:17|         162|\n",
      "|2021-01-10 00:55:17|         256|2021-01-10 01:04:55|         144|\n",
      "|2021-01-01 01:14:38|         256|2021-01-01 01:17:29|         255|\n",
      "|2021-01-16 12:11:12|         256|2021-01-16 12:29:12|         211|\n",
      "|2021-01-23 22:24:56|         256|2021-01-23 23:05:06|         265|\n",
      "|2021-01-07 12:07:19|         256|2021-01-07 12:13:19|         255|\n",
      "|2021-01-10 22:28:58|         256|2021-01-10 22:32:56|          17|\n",
      "|2021-01-02 00:54:33|         256|2021-01-02 01:03:45|          97|\n",
      "|2021-01-21 01:39:28|         256|2021-01-21 01:51:21|         198|\n",
      "|2021-01-08 17:08:16|         256|2021-01-08 17:30:14|          79|\n",
      "|2021-01-13 22:57:21|         256|2021-01-13 23:10:16|         181|\n",
      "|2021-01-29 21:59:41|         256|2021-01-29 22:01:46|         255|\n",
      "+-------------------+------------+-------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# action show() after transformations\n",
    "df.select(\"pickup_datetime\", \"PULocationID\", \"dropoff_datetime\", \"DOLocationID\") \\\n",
    "    .filter(df.PULocationID == 256).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c215e5da-a39b-4c95-aa9a-5e252fc5ad93",
   "metadata": {},
   "source": [
    "## Functions and User Defined Functions (UDFs)\n",
    "\n",
    "Besides the SQL and Pandas-like commands we've seen so far, Spark provides additional built-in functions that allow for more complex data manipulation. By convention, these functions are imported as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e04797af-3a1e-47ab-8d3f-0e6c4a19eff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a332278-54a4-4275-b1c9-957466bc882c",
   "metadata": {},
   "source": [
    "Example of built-in functions usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eda4e472-cfa8-42c0-b6d6-d24c484965f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+------------+------------+\n",
      "|pickup_date|PULocationID|dropoff_date|DOLocationID|\n",
      "+-----------+------------+------------+------------+\n",
      "| 2021-01-25|         247|  2021-01-25|         169|\n",
      "| 2021-01-01|          14|  2021-01-01|         227|\n",
      "| 2021-01-29|         134|  2021-01-29|         138|\n",
      "| 2021-01-12|           4|  2021-01-12|         137|\n",
      "| 2021-01-26|           7|  2021-01-26|         239|\n",
      "| 2021-01-22|         242|  2021-01-22|          78|\n",
      "| 2021-01-23|          80|  2021-01-23|         148|\n",
      "| 2021-01-19|         244|  2021-01-19|         128|\n",
      "| 2021-01-12|         230|  2021-01-12|         225|\n",
      "| 2021-01-20|         228|  2021-01-20|          26|\n",
      "| 2021-01-29|         123|  2021-01-29|         130|\n",
      "| 2021-01-16|          47|  2021-01-16|         119|\n",
      "| 2021-01-03|          14|  2021-01-03|          75|\n",
      "| 2021-01-28|          61|  2021-01-28|          61|\n",
      "| 2021-01-10|         150|  2021-01-10|         150|\n",
      "| 2021-01-30|         205|  2021-01-30|         205|\n",
      "| 2021-01-15|         129|  2021-01-15|         145|\n",
      "| 2021-01-14|         157|  2021-01-14|         236|\n",
      "| 2021-01-22|          76|  2021-01-22|          91|\n",
      "| 2021-01-22|          41|  2021-01-22|          42|\n",
      "+-----------+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .withColumn(\"pickup_date\", F.to_date(df.pickup_datetime)) \\\n",
    "    .withColumn(\"dropoff_date\", F.to_date(df.dropoff_datetime)) \\\n",
    "    .select(\"pickup_date\", \"PULocationID\", \"dropoff_date\", \"DOLocationID\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860f1df5-0d04-4864-8597-d1f6f59304e7",
   "metadata": {},
   "source": [
    "* **`withColumn()`**: adds a new column to the dataframe.\n",
    "* **`F.to_date()`**: converts timestamp to date format.\n",
    "\n",
    "Find the list of built-in functions in the [Spark documentation](https://spark.apache.org/docs/latest/api/sql/index.html).\n",
    "\n",
    "We can also create our own **User Defined Functions (UDFs)**. UDFs are regular functions which are then passed as parameters to a special builder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eec98e80-2155-40d7-94b0-ffb5c16c2f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that changes values when they're divisible by 7 or 3\n",
    "def crazy_stuff(base_num):\n",
    "    num = int(base_num[1:])\n",
    "    if num % 7 == 0:\n",
    "        return f\"s/{num:03x}\"\n",
    "    elif num % 3 == 0:\n",
    "        return f\"a/{num:03x}\"\n",
    "    else:\n",
    "        return f\"e/{num:03x}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6306bac-2224-47a1-b107-f0005035b1e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a/a7a'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crazy_stuff(\"B02682\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5167951-f593-4bf6-acea-88b2413a5c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the previous regular function into a UDF\n",
    "crazy_stuff_udf = F.udf(crazy_stuff, returnType=StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50346bf0-0892-4d4a-be18-0a9f8233f6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------+------------+------------+\n",
      "|base_id|pickup_date|PULocationID|dropoff_date|DOLocationID|\n",
      "+-------+-----------+------------+------------+------------+\n",
      "|  e/9ce| 2021-01-25|         247|  2021-01-25|         169|\n",
      "|  e/9ce| 2021-01-01|          14|  2021-01-01|         227|\n",
      "|  e/9ce| 2021-01-29|         134|  2021-01-29|         138|\n",
      "|  e/b32| 2021-01-12|           4|  2021-01-12|         137|\n",
      "|  e/9ce| 2021-01-26|           7|  2021-01-26|         239|\n",
      "|  e/b32| 2021-01-22|         242|  2021-01-22|          78|\n",
      "|  e/9ce| 2021-01-23|          80|  2021-01-23|         148|\n",
      "|  e/acc| 2021-01-19|         244|  2021-01-19|         128|\n",
      "|  e/b38| 2021-01-12|         230|  2021-01-12|         225|\n",
      "|  s/b44| 2021-01-20|         228|  2021-01-20|          26|\n",
      "|  e/b32| 2021-01-29|         123|  2021-01-29|         130|\n",
      "|  e/b3e| 2021-01-16|          47|  2021-01-16|         119|\n",
      "|  e/b32| 2021-01-03|          14|  2021-01-03|          75|\n",
      "|  e/9ce| 2021-01-28|          61|  2021-01-28|          61|\n",
      "|  e/b3b| 2021-01-10|         150|  2021-01-10|         150|\n",
      "|  e/acc| 2021-01-30|         205|  2021-01-30|         205|\n",
      "|  s/acd| 2021-01-15|         129|  2021-01-15|         145|\n",
      "|  e/b47| 2021-01-14|         157|  2021-01-14|         236|\n",
      "|  e/9ce| 2021-01-22|          76|  2021-01-22|          91|\n",
      "|  e/b38| 2021-01-22|          41|  2021-01-22|          42|\n",
      "+-------+-----------+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# apply the UDF to our dataframe\n",
    "df \\\n",
    "    .withColumn(\"pickup_date\", F.to_date(df.pickup_datetime)) \\\n",
    "    .withColumn(\"dropoff_date\", F.to_date(df.dropoff_datetime)) \\\n",
    "    .withColumn(\"base_id\", crazy_stuff_udf(df.dispatching_base_num)) \\\n",
    "    .select(\"base_id\", \"pickup_date\", \"PULocationID\", \"dropoff_date\", \"DOLocationID\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f40e7ab-ab05-4c64-ae7d-949d81dbd63e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
