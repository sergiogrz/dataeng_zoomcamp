{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3009f81-add6-42ab-bb69-6ff85ac90c7c",
   "metadata": {},
   "source": [
    "# Resilient Distributed Datasets (RDDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc356806-2bcc-4131-92ae-59e0f8cbec73",
   "metadata": {},
   "source": [
    "## RDDs simple operations: map and reduce\n",
    "\n",
    "Dataframes are implemented on top of another data structure called **Resilient Distributed Dataset**, or **RDD**, which is a lower level abstraction.\n",
    "\n",
    "Since Spark added support for dataframes and SQL, in general there is no need to work directly with RDDs, which is more complex and time-consuming. But knowing how they work can help us understand how to make better use of Spark.\n",
    "\n",
    "One difference is that dataframes follow a specific structure, have a schema. On the contrary RDDs have **no schema**, they are just a distributed collection of objects partitioned accross the nodes of the cluster."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "01671025-5e1a-4659-ad7e-ec857d4b0ead",
   "metadata": {},
   "source": [
    "### Reproduce query with RDDs\n",
    "\n",
    "We want to implement with RDDs the following query from `04_groupby_and_joins.ipynb` notebook.\n",
    "\n",
    "```\n",
    "SELECT \n",
    "    date_trunc('hour', lpep_pickup_datetime) AS hour,\n",
    "    PULocationID AS revenue_zone,\n",
    "    SUM(total_amount) AS amount,\n",
    "    COUNT(1) AS number_records\n",
    "FROM\n",
    "    green_data\n",
    "WHERE lpep_pickup_datetime >= '2020-01-01 00:00:00'\n",
    "GROUP BY\n",
    "    1, 2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57672d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "971b1728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/23 18:07:07 WARN Utils: Your hostname, GRAD0365UBUNTU resolves to a loopback address: 127.0.1.1; using 192.168.68.103 instead (on interface wlp0s20f3)\n",
      "24/02/23 18:07:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/23 18:07:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/02/23 18:07:08 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"rdds\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4c149ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green = spark.read.parquet(\"../data/pq/green/*/*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a512451f",
   "metadata": {},
   "source": [
    "### From dataframe to RDD\n",
    "\n",
    "Spark dataframes have an attribute called **rdd**, which contains the raw RDD of the dataframe. The RDD's objects used for building the dataframe are called **Rows**.\n",
    "\n",
    "Below we apply several useful functions:\n",
    "* **`filter()`** returns a new RDD cointaining only the elements that satisfy a predicate.\n",
    "* **`map()`** takes an RDD as input, transforms it with a function and returns a new RDD.\n",
    "* **`reduceByKey()`** method, which will take all records with the same key and put them together in a single record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a378200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[7] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_green.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb041e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 12, 18, 15, 4), lpep_dropoff_datetime=datetime.datetime(2020, 1, 12, 18, 19, 52), store_and_fwd_flag='N', RatecodeID=1, PULocationID=41, DOLocationID=41, passenger_count=1, trip_distance=0.78, fare_amount=5.5, extra=0.0, mta_tax=0.5, tip_amount=1.58, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=7.88, payment_type=1, trip_type=1, congestion_surcharge=0.0),\n",
       " Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 31, 20, 24, 10), lpep_dropoff_datetime=datetime.datetime(2020, 1, 31, 20, 31, 51), store_and_fwd_flag='N', RatecodeID=1, PULocationID=173, DOLocationID=70, passenger_count=1, trip_distance=0.98, fare_amount=7.0, extra=0.5, mta_tax=0.5, tip_amount=0.0, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=8.3, payment_type=2, trip_type=1, congestion_surcharge=0.0),\n",
       " Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 7, 8, 16, 53), lpep_dropoff_datetime=datetime.datetime(2020, 1, 7, 8, 41, 39), store_and_fwd_flag='N', RatecodeID=1, PULocationID=74, DOLocationID=236, passenger_count=1, trip_distance=2.7, fare_amount=16.0, extra=0.0, mta_tax=0.5, tip_amount=3.91, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=23.46, payment_type=1, trip_type=1, congestion_surcharge=2.75)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_green.rdd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c4a6bb",
   "metadata": {},
   "source": [
    "### Filter\n",
    "\n",
    "Let's select only the columns we need for our query, and then filter based on the WHERE clause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5ee738c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 12, 18, 15, 4), PULocationID=41, total_amount=7.88),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 31, 20, 24, 10), PULocationID=173, total_amount=8.3),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 7, 8, 16, 53), PULocationID=74, total_amount=23.46)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = df_green \\\n",
    "    .select(\"lpep_pickup_datetime\", \"PULocationID\", \"total_amount\") \\\n",
    "    .rdd\n",
    "\n",
    "rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0f9b158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 12, 18, 15, 4), PULocationID=41, total_amount=7.88),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 31, 20, 24, 10), PULocationID=173, total_amount=8.3),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 7, 8, 16, 53), PULocationID=74, total_amount=23.46)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "start = datetime(year=2020, month=1, day=1)\n",
    "\n",
    "def filter_outliers(row):\n",
    "    return row.lpep_pickup_datetime >= start\n",
    "\n",
    "rdd.filter(filter_outliers).take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd797dfb",
   "metadata": {},
   "source": [
    "### Map\n",
    "\n",
    "We need to generate intermediate results in a very similar way to the original SQL query, so we will need to create the composite key `(hour, revenue_zone)` and a composite value `(amount, count)`, which are the 2 halves of each record that the executors will generate:\n",
    "```\n",
    "Each record: (key, value) = ((hour, revenue_zone), (amount, count))\n",
    "```\n",
    "Once we have a function that generates the record, we will use the **`map()`** method, which takes an RDD, transforms it with a function (our key-value function) and returns a new RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54ecffd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 12, 18, 15, 4), PULocationID=41, total_amount=7.88)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = rdd.take(1)[0]\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79afcbbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2020, 1, 12, 18, 0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row.lpep_pickup_datetime.replace(minute=0, second=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "630dcb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_grouping(row):\n",
    "    # key = (hour, revenue_zone)\n",
    "    hour = row.lpep_pickup_datetime.replace(minute=0, second=0)\n",
    "    revenue_zone = row.PULocationID\n",
    "    key = (hour, revenue_zone)\n",
    "\n",
    "    # value = (amount, count)\n",
    "    amount = row.total_amount\n",
    "    count = 1\n",
    "    value = (amount, count)\n",
    "    \n",
    "    return (key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "714cad0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((datetime.datetime(2020, 1, 12, 18, 0), 41), (7.88, 1)),\n",
       " ((datetime.datetime(2020, 1, 31, 20, 0), 173), (8.3, 1)),\n",
       " ((datetime.datetime(2020, 1, 7, 8, 0), 74), (23.46, 1)),\n",
       " ((datetime.datetime(2020, 1, 15, 14, 0), 25), (7.3, 1)),\n",
       " ((datetime.datetime(2020, 1, 31, 10, 0), 259), (25.54, 1))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd \\\n",
    "    .filter(filter_outliers) \\\n",
    "    .map(prepare_for_grouping) \\\n",
    "    .take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54f7171",
   "metadata": {},
   "source": [
    "### ReduceByKey\n",
    "\n",
    "We now need to use the **`reduceByKey()`** method, which will reduce all the records with the same key to a single record. Since we want to count the total amount and the total number of records, we just need to add the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d76228a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_revenue(left_value, right_value):\n",
    "    left_amount, left_count = left_value\n",
    "    right_amount, right_count = right_value\n",
    "    \n",
    "    output_amount = left_amount + right_amount\n",
    "    output_count = left_count + right_count\n",
    "    \n",
    "    return (output_amount, output_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2206d0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[((datetime.datetime(2020, 1, 13, 10, 0), 165), (55.43000000000001, 3)),\n",
       " ((datetime.datetime(2020, 1, 23, 10, 0), 43), (315.40000000000003, 19)),\n",
       " ((datetime.datetime(2020, 1, 2, 9, 0), 116), (296.54, 17)),\n",
       " ((datetime.datetime(2020, 1, 31, 21, 0), 41), (588.1599999999999, 40)),\n",
       " ((datetime.datetime(2020, 1, 26, 0, 0), 258), (56.92, 1))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd \\\n",
    "    .filter(filter_outliers) \\\n",
    "    .map(prepare_for_grouping) \\\n",
    "    .reduceByKey(calculate_revenue) \\\n",
    "    .take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31791d4e",
   "metadata": {},
   "source": [
    "### From RDD to dataframe\n",
    "\n",
    "The nested structure above is not very useful to work with. To turn the structure back to a dataframe we first apply another **`map()`** (to unnest what we got after the reduce operation) and **`toDF()`** method with the desired schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eefeae89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unwrap(row):\n",
    "    return (row[0][0], row[0][1], row[1][0], row[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67a08e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(datetime.datetime(2020, 1, 13, 10, 0), 165, 55.43000000000001, 3),\n",
       " (datetime.datetime(2020, 1, 23, 10, 0), 43, 315.40000000000003, 19),\n",
       " (datetime.datetime(2020, 1, 2, 9, 0), 116, 296.54, 17),\n",
       " (datetime.datetime(2020, 1, 31, 21, 0), 41, 588.1599999999999, 40),\n",
       " (datetime.datetime(2020, 1, 26, 0, 0), 258, 56.92, 1)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd \\\n",
    "    .filter(filter_outliers) \\\n",
    "    .map(prepare_for_grouping) \\\n",
    "    .reduceByKey(calculate_revenue) \\\n",
    "    .map(unwrap) \\\n",
    "    .take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9187a510",
   "metadata": {},
   "source": [
    "Finally, we turn the structure back to a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fbc69e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+------------------+---+\n",
      "|                 _1| _2|                _3| _4|\n",
      "+-------------------+---+------------------+---+\n",
      "|2020-01-13 10:00:00|165| 55.43000000000001|  3|\n",
      "|2020-01-23 10:00:00| 43|315.40000000000003| 19|\n",
      "|2020-01-02 09:00:00|116|            296.54| 17|\n",
      "|2020-01-31 21:00:00| 41| 588.1599999999999| 40|\n",
      "|2020-01-26 00:00:00|258|             56.92|  1|\n",
      "+-------------------+---+------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd \\\n",
    "    .filter(filter_outliers) \\\n",
    "    .map(prepare_for_grouping) \\\n",
    "    .reduceByKey(calculate_revenue) \\\n",
    "    .map(unwrap) \\\n",
    "    .toDF() \\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465aa15f",
   "metadata": {},
   "source": [
    "The column names have been lost in the first `map()` operation, so we can do it a bit differently to keep the names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c69f09ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "RevenueRow = namedtuple(\"RevenueRow\", [\"hour\", \"revenue_zone\", \"revenue\", \"count\"])\n",
    "\n",
    "def unwrap(row):\n",
    "    return RevenueRow(\n",
    "        hour=row[0][0],\n",
    "        revenue_zone=row[0][1],\n",
    "        revenue=row[1][0], \n",
    "        count=row[1][1]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e85cf4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_result = rdd \\\n",
    "    .filter(filter_outliers) \\\n",
    "    .map(prepare_for_grouping) \\\n",
    "    .reduceByKey(calculate_revenue) \\\n",
    "    .map(unwrap) \\\n",
    "    .toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e5df587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+------------------+-----+\n",
      "|               hour|revenue_zone|           revenue|count|\n",
      "+-------------------+------------+------------------+-----+\n",
      "|2020-01-13 10:00:00|         165| 55.43000000000001|    3|\n",
      "|2020-01-23 10:00:00|          43|315.40000000000003|   19|\n",
      "|2020-01-02 09:00:00|         116|            296.54|   17|\n",
      "|2020-01-31 21:00:00|          41| 588.1599999999999|   40|\n",
      "|2020-01-26 00:00:00|         258|             56.92|    1|\n",
      "+-------------------+------------+------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e1e2fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('hour', TimestampType(), True), StructField('revenue_zone', LongType(), True), StructField('revenue', DoubleType(), True), StructField('count', LongType(), True)])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3efc7f5",
   "metadata": {},
   "source": [
    "We want to change the schema to the one below:\n",
    "\n",
    "```\n",
    "StructType([\n",
    "    StructField('hour', TimestampType(), True),\n",
    "    StructField('revenue_zone', IntegerType(), True),\n",
    "    StructField('revenue', DoubleType(), True),\n",
    "    StructField('count', IntegerType(), True)\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86f04124",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, TimestampType, IntegerType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20050e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_schema = StructType([\n",
    "    StructField('hour', TimestampType(), True),\n",
    "    StructField('revenue_zone', IntegerType(), True),\n",
    "    StructField('revenue', DoubleType(), True),\n",
    "    StructField('count', IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff43f984",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = rdd \\\n",
    "    .filter(filter_outliers) \\\n",
    "    .map(prepare_for_grouping) \\\n",
    "    .reduceByKey(calculate_revenue) \\\n",
    "    .map(unwrap) \\\n",
    "    .toDF(result_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9cab7c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+------------------+-----+\n",
      "|               hour|revenue_zone|           revenue|count|\n",
      "+-------------------+------------+------------------+-----+\n",
      "|2020-01-13 10:00:00|         165| 55.43000000000001|    3|\n",
      "|2020-01-23 10:00:00|          43|315.40000000000003|   19|\n",
      "|2020-01-02 09:00:00|         116|            296.54|   17|\n",
      "|2020-01-31 21:00:00|          41| 588.1599999999999|   40|\n",
      "|2020-01-26 00:00:00|         258|             56.92|    1|\n",
      "+-------------------+------------+------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e315ca3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hour: timestamp (nullable = true)\n",
      " |-- revenue_zone: integer (nullable = true)\n",
      " |-- revenue: double (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_result.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d714fe3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/23 18:43:20 WARN MemoryManager: Total allocation exceeds 95,00% (906.992.014 bytes) of heap memory\n",
      "Scaling row group sizes to 96,54% for 7 writers\n",
      "24/02/23 18:43:20 WARN MemoryManager: Total allocation exceeds 95,00% (906.992.014 bytes) of heap memory\n",
      "Scaling row group sizes to 84,47% for 8 writers\n",
      "24/02/23 18:43:22 WARN MemoryManager: Total allocation exceeds 95,00% (906.992.014 bytes) of heap memory\n",
      "Scaling row group sizes to 96,54% for 7 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_result.write.parquet(\"../data/tmp/green-revenue\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eeac9a",
   "metadata": {},
   "source": [
    "## Spark RDD mapPartitions\n",
    "\n",
    "Similar to **`map()`** function, **`mapPartitions()`** gets in a partition and outputs another partition. However, they differ in the process:\n",
    "* `map()` gets in an RDD, it is applied to every element of this RDD, and for every element it produces another element and in the end it creates another RDD.\n",
    "* `mapPartitions()` gets a partition (RDD), applies a function to the entire partition or chunk of data, and outputs another partition (RDD).\n",
    "\n",
    "Thanks to this, `mapPartitions()` is a convenient method for dealing with large datasets, since it allows us to separate them into chunks that we can process more easily. For applications such as Machine Learning this behavior is very useful. In this case, if we pass the ML model to `mapPartitions()`, it will apply it to each chunk and combine the results.\n",
    "\n",
    "Let's imagine we want to create an application or a service that predicts the duration of a trip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "056240c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- lpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- lpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- ehail_fee: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- trip_type: integer (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_green.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec135ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+------------+------------+-------------+\n",
      "|VendorID|lpep_pickup_datetime|PULocationID|DOLocationID|trip_distance|\n",
      "+--------+--------------------+------------+------------+-------------+\n",
      "|       2| 2020-01-12 18:15:04|          41|          41|         0.78|\n",
      "|       2| 2020-01-31 20:24:10|         173|          70|         0.98|\n",
      "|       2| 2020-01-07 08:16:53|          74|         236|          2.7|\n",
      "|       1| 2020-01-15 14:47:15|          25|          66|          0.8|\n",
      "|    null| 2020-01-31 10:08:00|         259|          51|         2.33|\n",
      "+--------+--------------------+------------+------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"VendorID\", \"lpep_pickup_datetime\", \"PULocationID\", \"DOLocationID\", \"trip_distance\"]\n",
    "\n",
    "df_green \\\n",
    "    .select(columns) \\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb935a20",
   "metadata": {},
   "source": [
    "To apply a ML model, we convert the dataframe to an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f1cce76a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 12, 18, 15, 4), PULocationID=41, DOLocationID=41, trip_distance=0.78),\n",
       " Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 31, 20, 24, 10), PULocationID=173, DOLocationID=70, trip_distance=0.98),\n",
       " Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 7, 8, 16, 53), PULocationID=74, DOLocationID=236, trip_distance=2.7),\n",
       " Row(VendorID=1, lpep_pickup_datetime=datetime.datetime(2020, 1, 15, 14, 47, 15), PULocationID=25, DOLocationID=66, trip_distance=0.8),\n",
       " Row(VendorID=None, lpep_pickup_datetime=datetime.datetime(2020, 1, 31, 10, 8), PULocationID=259, DOLocationID=51, trip_distance=2.33)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration_rdd = df_green \\\n",
    "    .select(columns) \\\n",
    "    .rdd\n",
    "duration_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f10a624e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[746744, 418184, 219219, 215720, 212420, 199320, 183795, 109115]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple example of how mapPartitions works\n",
    "# count the lines in each partition\n",
    "def apply_model_in_batch(partition):\n",
    "    cnt = 0\n",
    "    for row in partition:\n",
    "        cnt += 1\n",
    "    \n",
    "    return [cnt]\n",
    "\n",
    "duration_rdd.mapPartitions(apply_model_in_batch).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6ef6424a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[746744, 418184, 219219, 215720, 212420, 199320, 183795, 109115]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def apply_model_in_batch(rows):\n",
    "    # convert the partition to a pandas DataFrame\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "    cnt = len(df)\n",
    "\n",
    "    return [cnt]\n",
    "\n",
    "duration_rdd.mapPartitions(apply_model_in_batch).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b06bb259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ...\n",
    "def model_predict(df):\n",
    "    # y_pred = model.predict(df)\n",
    "    y_pred = df.trip_distance * 5\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2a08be77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model_in_batch(rows):\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "    predictions = model_predict(df)\n",
    "    df[\"predicted_duration\"] = predictions\n",
    "    \n",
    "    for row in df.itertuples():\n",
    "        yield row\n",
    "\n",
    "    return [cnt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "72ceaef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Pandas(Index=0, VendorID=2.0, lpep_pickup_datetime=Timestamp('2020-01-12 18:15:04'), PULocationID=41, DOLocationID=41, trip_distance=0.78, predicted_duration=3.9000000000000004),\n",
       " Pandas(Index=1, VendorID=2.0, lpep_pickup_datetime=Timestamp('2020-01-31 20:24:10'), PULocationID=173, DOLocationID=70, trip_distance=0.98, predicted_duration=4.9),\n",
       " Pandas(Index=2, VendorID=2.0, lpep_pickup_datetime=Timestamp('2020-01-07 08:16:53'), PULocationID=74, DOLocationID=236, trip_distance=2.7, predicted_duration=13.5),\n",
       " Pandas(Index=3, VendorID=1.0, lpep_pickup_datetime=Timestamp('2020-01-15 14:47:15'), PULocationID=25, DOLocationID=66, trip_distance=0.8, predicted_duration=4.0),\n",
       " Pandas(Index=4, VendorID=nan, lpep_pickup_datetime=Timestamp('2020-01-31 10:08:00'), PULocationID=259, DOLocationID=51, trip_distance=2.33, predicted_duration=11.65),\n",
       " Pandas(Index=5, VendorID=2.0, lpep_pickup_datetime=Timestamp('2020-01-18 17:46:45'), PULocationID=177, DOLocationID=188, trip_distance=2.62, predicted_duration=13.100000000000001),\n",
       " Pandas(Index=6, VendorID=2.0, lpep_pickup_datetime=Timestamp('2020-01-17 20:08:44'), PULocationID=65, DOLocationID=97, trip_distance=1.13, predicted_duration=5.6499999999999995),\n",
       " Pandas(Index=7, VendorID=nan, lpep_pickup_datetime=Timestamp('2020-01-13 10:47:00'), PULocationID=165, DOLocationID=123, trip_distance=1.36, predicted_duration=6.800000000000001),\n",
       " Pandas(Index=8, VendorID=nan, lpep_pickup_datetime=Timestamp('2020-01-07 15:36:00'), PULocationID=170, DOLocationID=220, trip_distance=11.15, predicted_duration=55.75),\n",
       " Pandas(Index=9, VendorID=nan, lpep_pickup_datetime=Timestamp('2020-01-10 11:47:00'), PULocationID=74, DOLocationID=41, trip_distance=1.78, predicted_duration=8.9)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration_rdd.mapPartitions(apply_model_in_batch).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0c89f8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_predicts = duration_rdd \\\n",
    "    .mapPartitions(apply_model_in_batch) \\\n",
    "    .toDF() \\\n",
    "    .drop(\"Index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ccb93bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+------------+------------+-------------+------------------+\n",
      "|VendorID|lpep_pickup_datetime|PULocationID|DOLocationID|trip_distance|predicted_duration|\n",
      "+--------+--------------------+------------+------------+-------------+------------------+\n",
      "|     2.0|                  {}|          41|          41|         0.78|3.9000000000000004|\n",
      "|     2.0|                  {}|         173|          70|         0.98|               4.9|\n",
      "|     2.0|                  {}|          74|         236|          2.7|              13.5|\n",
      "|     1.0|                  {}|          25|          66|          0.8|               4.0|\n",
      "|     NaN|                  {}|         259|          51|         2.33|             11.65|\n",
      "|     2.0|                  {}|         177|         188|         2.62|13.100000000000001|\n",
      "|     2.0|                  {}|          65|          97|         1.13|5.6499999999999995|\n",
      "|     NaN|                  {}|         165|         123|         1.36| 6.800000000000001|\n",
      "|     NaN|                  {}|         170|         220|        11.15|             55.75|\n",
      "|     NaN|                  {}|          74|          41|         1.78|               8.9|\n",
      "|     1.0|                  {}|          75|          41|          1.0|               5.0|\n",
      "|     2.0|                  {}|          74|         151|         2.75|             13.75|\n",
      "|     1.0|                  {}|          41|          74|          1.1|               5.5|\n",
      "|     2.0|                  {}|         116|          74|         3.81|             19.05|\n",
      "|     2.0|                  {}|         152|         244|         1.85|              9.25|\n",
      "|     NaN|                  {}|          71|          88|         9.14|              45.7|\n",
      "|     2.0|                  {}|          43|         236|         1.04|               5.2|\n",
      "|     2.0|                  {}|          65|          49|         1.14| 5.699999999999999|\n",
      "|     NaN|                  {}|         254|         254|         1.15|              5.75|\n",
      "|     2.0|                  {}|         116|         244|         0.92|4.6000000000000005|\n",
      "+--------+--------------------+------------+------------+-------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_predicts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f18c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
