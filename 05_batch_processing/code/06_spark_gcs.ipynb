{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b35d946",
   "metadata": {},
   "source": [
    "# Connecting local Spark to GCS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2806e01c",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80ef53e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9346c692",
   "metadata": {},
   "source": [
    "## Configure Spark\n",
    "\n",
    "### SparkConf - configuration object\n",
    "\n",
    "Instead of just using `builder` attribute, in this case we need to provide some configuration. We create a **`SparkConf()`** configuration object, where we use the cluster in local mode, set the application name, and specify some settings, such as the locations of both the connector jar file and the google credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "502c842e",
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials_location = \"/home/sgrodriguez/.google/credentials/google_credentials.json\"\n",
    "\n",
    "conf = SparkConf() \\\n",
    "    .setMaster(\"local[*]\") \\\n",
    "    .setAppName(\"connect local spark to gcs\") \\\n",
    "    .set(\"spark.jars\", \"../../lib/gcs-connector-hadoop3-2.2.20.jar\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", credentials_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06400b17",
   "metadata": {},
   "source": [
    "### SparkContext\n",
    "\n",
    "Unlike in previous notebooks, where we implicitly had created a context (which represents a connection to a Spark cluster) when running `SparkSession.builder`, this time we need to explicitly create and configure the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "241fb43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/24 11:06:53 WARN Utils: Your hostname, GRAD0365UBUNTU resolves to a loopback address: 127.0.1.1; using 192.168.68.103 instead (on interface wlp0s20f3)\n",
      "24/02/24 11:06:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/02/24 11:06:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "\n",
    "hadoop_conf.set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "hadoop_conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.json.keyfile\", credentials_location)\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.enable\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90982255",
   "metadata": {},
   "source": [
    "### SparkSession\n",
    "\n",
    "We can now instantiate a SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc0e4574",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(conf=sc.getConf()) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18790044",
   "metadata": {},
   "source": [
    "## Check connection to GCS\n",
    "\n",
    "Let's check if our connection to GCS is successful by reading some of the Parquet files we have in our bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6efb3b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_green = spark.read.parquet(\"gs://dataeng_zoomcamp_bucket_warm-rock-411419/spark/pq/green/*/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae1618a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "|VendorID|lpep_pickup_datetime|lpep_dropoff_datetime|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|ehail_fee|improvement_surcharge|total_amount|payment_type|trip_type|congestion_surcharge|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "|       2| 2020-01-12 18:15:04|  2020-01-12 18:19:52|                 N|         1|          41|          41|              1|         0.78|        5.5|  0.0|    0.5|      1.58|         0.0|     null|                  0.3|        7.88|           1|        1|                 0.0|\n",
      "|       2| 2020-01-31 20:24:10|  2020-01-31 20:31:51|                 N|         1|         173|          70|              1|         0.98|        7.0|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|         8.3|           2|        1|                 0.0|\n",
      "|       2| 2020-01-07 08:16:53|  2020-01-07 08:41:39|                 N|         1|          74|         236|              1|          2.7|       16.0|  0.0|    0.5|      3.91|         0.0|     null|                  0.3|       23.46|           1|        1|                2.75|\n",
      "|       1| 2020-01-15 14:47:15|  2020-01-15 14:54:34|                 N|         1|          25|          66|              1|          0.8|        6.5|  0.0|    0.5|       0.0|         0.0|     null|                  0.3|         7.3|           2|        1|                 0.0|\n",
      "|    null| 2020-01-31 10:08:00|  2020-01-31 10:20:00|              null|      null|         259|          51|           null|         2.33|      22.49| 2.75|    0.0|       0.0|         0.0|     null|                  0.3|       25.54|        null|     null|                null|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_green.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d6e78f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2304517"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_green.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9488e197",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
